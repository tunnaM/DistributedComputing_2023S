{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ca3697",
   "metadata": {},
   "source": [
    "# PySpark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd303e5e",
   "metadata": {},
   "source": [
    "### 1. 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330a894",
   "metadata": {},
   "source": [
    "配置和启动 PySpark："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d9bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000022917548040>\n",
      "<SparkContext master=local[*] appName=PySpark RDD>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# 本地模式\n",
    "spark = SparkSession.builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"PySpark RDD\").\\\n",
    "    getOrCreate()\n",
    "sc = spark.sparkContext #存成变量，后面反复调用\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963646ff",
   "metadata": {},
   "source": [
    "### 2. RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946dd8e",
   "metadata": {},
   "source": [
    "创建一个包含了数据的列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99e954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "vec = [math.sin(i + math.exp(i)) for i in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b5dbc",
   "metadata": {},
   "source": [
    "将其转换为分布式数据结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ed1a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = sc.parallelize(vec) # 基于网络传输方法 比直接调用数据慢\n",
    "dat # 迭代器思路，大数据处理思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf73ad",
   "metadata": {},
   "source": [
    "`dat` 的类型是 RDD（Resilient Distributed Dataset），是一种类似于迭代器的结构，可以看作是某种数据类型的容器。例如，`dat` 代表了一些数字的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407940c",
   "metadata": {},
   "source": [
    "类似于 Python 中原生的函数式编程工具，可以在 RDD 中使用 Map/Filter/Reduce 等操作。例如计算求和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9795213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.246114436451575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.reduce(lambda x, y: x + y) # pyspark比在python简洁，不需要Import，面向对象编程思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bff19",
   "metadata": {},
   "source": [
    "灵活使用 Map 函数计算均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350aecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat2 = dat.map(lambda x: (1, x))\n",
    "dat2 # 将RDD当做容器，生成新的RDD，可以调用reduce，可以写在一行\n",
    "# 能否接着套用，原则在于是否返回RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192de0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, -2.246114436451575)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.map(lambda x: (1, x)).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1])) # 同时获得样本量和求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792ff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写一个生成方差\n",
    "dat3 = dat.map(lambda x: (1, x, x*x)).reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b1aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4639116338164096\n"
     ]
    }
   ],
   "source": [
    "mean = dat3[1] / dat3[0]\n",
    "vec_var = (dat3[2] - dat3[0] * mean * mean) / (dat3[0] - 1)\n",
    "print(vec_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059cab6",
   "metadata": {},
   "source": [
    "Filter 操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d253c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.015734417272107"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.filter(lambda x: x > 0).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b0d217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-30.261848853723677"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.filter(lambda x: x <= 0).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eb3a4",
   "metadata": {},
   "source": [
    "使用 `collect()` 函数可以将 RDD 中的数据全部取出返回给 Python，**但对于大型数据请谨慎操作！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7ba9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8414709848078965,\n",
       " -0.5452515566923345,\n",
       " 0.035714265168052234,\n",
       " -0.8886479175586053,\n",
       " 0.8876009615390265,\n",
       " 0.5011099612213634,\n",
       " 0.8530218378956966,\n",
       " -0.804086324216863,\n",
       " -0.9644551022640215,\n",
       " 0.4721216528877472,\n",
       " 0.9723105121477627,\n",
       " 0.10237280614077035,\n",
       " 0.7682074937713861,\n",
       " 0.819078842327986,\n",
       " -0.7885252480190347,\n",
       " -0.8483650910372161,\n",
       " -0.2445565303431489,\n",
       " -0.8558519039634782,\n",
       " -0.10196456500793882,\n",
       " 0.14618338451195673,\n",
       " 0.9999672698820121,\n",
       " -0.07924816445805015,\n",
       " -0.10590380349630102,\n",
       " -0.5358980946771431,\n",
       " -0.3136640428099456,\n",
       " 0.7777700593006833,\n",
       " 0.9146442256184393,\n",
       " 0.48464438753683886,\n",
       " 0.11080525077159722,\n",
       " -0.9656357654018414,\n",
       " 0.9737279913519716,\n",
       " 0.5319589257495517,\n",
       " 0.9956230914517219,\n",
       " 0.48682123564309765,\n",
       " -0.48489790225635604,\n",
       " 0.5791514931370622,\n",
       " 0.15744970934574964,\n",
       " -0.9772162984957723,\n",
       " -0.27703274027791913,\n",
       " -0.774633895198441,\n",
       " -0.655895619972691,\n",
       " -0.1021743587186752,\n",
       " 0.25698429832699954,\n",
       " 0.8617085926612364,\n",
       " -0.9899871580091143,\n",
       " -0.9970349352676108,\n",
       " -0.9972719456409868,\n",
       " 0.28667367857218606,\n",
       " 0.9914056689461122,\n",
       " 0.8758191086384202,\n",
       " 0.47110776655601394,\n",
       " 0.9915906593803753,\n",
       " 0.283585074390262,\n",
       " -0.9250746204217681,\n",
       " -0.2760627085070803,\n",
       " -0.9605297369398103,\n",
       " 0.27663703848924504,\n",
       " 0.8376007223520099,\n",
       " 0.31529766772832263,\n",
       " 0.0467250474926968,\n",
       " 0.7301221780683019,\n",
       " -0.019807702907643797,\n",
       " 0.28166848857595156,\n",
       " -0.04132408178025465,\n",
       " 0.9994178624383043,\n",
       " -0.3297882131661022,\n",
       " 0.05249861368120975,\n",
       " -0.4173590172239696,\n",
       " 0.16581933154694034,\n",
       " -0.8634792702714021,\n",
       " -0.4333571589785714,\n",
       " 0.8760424061455578,\n",
       " -0.9689218043045077,\n",
       " -0.41308613000578315,\n",
       " -0.08583325804146333,\n",
       " -0.8348331411895921,\n",
       " 0.9904096809452724,\n",
       " 0.6738518871018663,\n",
       " -0.6467442207097087,\n",
       " -0.5523253959215197,\n",
       " 0.0946425530668779,\n",
       " 0.9326207308098783,\n",
       " -0.9712614277963193,\n",
       " 0.0890088907150176,\n",
       " -0.15045386899932991,\n",
       " -0.33052751754365,\n",
       " -0.9823608103996693,\n",
       " -0.9632260215147505,\n",
       " -0.055780196761602924,\n",
       " 0.09016685881531959,\n",
       " -0.9949953391221849,\n",
       " -0.9561153559383516,\n",
       " -0.922656523771298,\n",
       " -0.14976755114026755,\n",
       " 0.27182393869080274,\n",
       " 0.9978615748967212,\n",
       " -0.9973589836219446,\n",
       " 0.07587629588596118,\n",
       " 0.7870114601458702,\n",
       " -0.7206198329616429]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.collect() # 一般不会使用，除非已知数据大小，很危险，知道最终长度固定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da68d7",
   "metadata": {},
   "source": [
    "RDD 还提供了许多便捷的函数，如 `count()` 用来计算数据/容器的大小，`take()` 返回前 `n` 个元素等等。完整的函数列表可以参考[官方文档](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a55387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.count() # 多少个元素，未必是一个行，可能把若干行打包成一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.take(5) # 数据前5行放在内存中，相当于小型collect，也可以用first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced4bdee",
   "metadata": {},
   "source": [
    "### 3. RDD 文件操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ee1cd",
   "metadata": {},
   "source": [
    "利用 Numpy 创建一个矩阵，并写入文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc36a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 100\n",
    "p = 5\n",
    "mat = np.random.normal(size=(n, p))\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "np.savetxt(\"data/mat_np.txt\", mat, fmt=\"%f\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44965a",
   "metadata": {},
   "source": [
    "PySpark 读取文件并进行一些简单操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dfebd79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "\n",
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "1.651437\t-2.426679\t-0.428913\t1.265936\t-0.866740\n",
      "-0.678886\t-0.094709\t1.491390\t-0.638902\t-0.443982\n",
      "-0.434351\t2.205930\t2.186786\t1.004054\t0.386186\n",
      "0.737369\t1.490732\t-0.935834\t1.175829\t-1.253881\n"
     ]
    }
   ],
   "source": [
    "file = sc.textFile(\"data/mat_np.txt\")\n",
    "\n",
    "# 打印矩阵行数\n",
    "print(file.count())\n",
    "\n",
    "# 空行\n",
    "print()\n",
    "\n",
    "# 打印前5行\n",
    "text = file.take(5) # 对数据格式有所了解\n",
    "print(*text, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a8bd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1.085631\\t0.997345\\t0.282978\\t-1.506295\\t-0.578600'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.first() # 本质还是一个字符串。真正需要的是里面的数字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4b35d",
   "metadata": {},
   "source": [
    "`file` 的类型也是 RDD。`file` 代表了一些字符串的集合，每个元素是矩阵文件中的一行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b646dc1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(file))\n",
    "print(type(file.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5ca80",
   "metadata": {},
   "source": [
    "我们可以对 RDD 进行变换，使一种元素类型的 RDD 变成另一种元素类型的 RDD。例如，将 `file` 中的每一个字符串变成一个 Numpy 向量，那么变换的结果就是以 Numpy.array 为类型的 RDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086a4827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1.085631\\t0.997345\\t0.282978\\t-1.506295\\t-0.578600'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = file.first()\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae03d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1.085631', '0.997345', '0.282978', '-1.506295', '-0.578600']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4d9c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"1.234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f1562",
   "metadata": {},
   "source": [
    "为此，我们需要先编写一个转换函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7366799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n"
     ]
    }
   ],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 将每一个元素从字符串变成数值型\n",
    "    num_vec = map(lambda s: float(s), str_vec) # 字符串映射\n",
    "    # 创建 Numpy 向量\n",
    "    return np.fromiter(num_vec, dtype=float)\n",
    "\n",
    "print(file.first())\n",
    "print(str_to_vec(file.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c2362",
   "metadata": {},
   "source": [
    "也可以让 Numpy 直接对字符串进行转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae3d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n"
     ]
    }
   ],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 让 Numpy 进行类型转换\n",
    "    return np.array(str_vec, dtype=float)\n",
    "\n",
    "print(file.first())\n",
    "print(str_to_vec(file.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e0bbe",
   "metadata": {},
   "source": [
    "生成新的 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b201abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dat = file.map(str_to_vec) \n",
    "print(type(dat)) # pipeline组合\n",
    "print(type(dat.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a88a30",
   "metadata": {},
   "source": [
    "RDD 的一般操作都支持："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc058e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n",
      "\n",
      "[array([-1.085631,  0.997345,  0.282978, -1.506295, -0.5786  ]), array([ 1.651437, -2.426679, -0.428913,  1.265936, -0.86674 ]), array([-0.678886, -0.094709,  1.49139 , -0.638902, -0.443982])]\n",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(dat.first())\n",
    "print()\n",
    "print(dat.take(3))\n",
    "print()\n",
    "print(dat.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31988da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.085631\t0.997345\t0.282978\t-1.506295\t-0.578600\n",
      "[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n"
     ]
    }
   ],
   "source": [
    "# str => np.array\n",
    "def str_to_vec(line):\n",
    "    # 分割字符串\n",
    "    str_vec = line.split(\"\\t\")\n",
    "    # 将每一个元素从字符串变成数值型\n",
    "    num_vec = map(lambda s: float(s), str_vec) # 字符串映射\n",
    "    # 创建 Numpy 向量\n",
    "    return np.fromiter(num_vec, dtype=float)\n",
    "print(file.first())\n",
    "print(str_to_vec(file.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b046a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0970826 ,  0.00832708, -0.03945197, -0.01719718, -0.04781526])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, xsum = sc.textFile(\"data/mat_np.txt\").\\\n",
    "  map(str_to_vec).\\\n",
    "  map(lambda x:(1,x)).\\\n",
    "  reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "xsum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aefeb8",
   "metadata": {},
   "source": [
    "### 4. RDD 分区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24f1a4",
   "metadata": {},
   "source": [
    "RDD 的一个重要功能在于可以分区（分块），从而支持分布式计算。查看 RDD 的分区数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1214a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.getNumPartitions() # 会根据数据大小自动分区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d0021",
   "metadata": {},
   "source": [
    "还可以手动指定分区数，从而支持更高的并行度。注意调用 `repartition()` 函数不改变原有 RDD，要使用分区后的 RDD，需要重新赋值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4819bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "file_p10 = file.repartition(10)\n",
    "print(file.getNumPartitions())\n",
    "print(file_p10.getNumPartitions()) # 增加并行度，生成一个新的RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582f8a5",
   "metadata": {},
   "source": [
    "我们可以按分区对数据进行转换，例如将每个分区的数据转成 Numpy 矩阵。需要使用的函数是 `mapPartitions()`，其接收一个函数作为参数，该函数将对每个分区的**迭代器**进行变换。某些分区可能会是空集，需要做特殊处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c697eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iter[str] => Iter[matrix]，以迭代器为输入，以迭代器为输出\n",
    "def part_to_mat(iterator): \n",
    "    # Iter[str] => Iter[np.array]\n",
    "    iter_arr = map(str_to_vec, iterator) # 用刚刚的函数，转换成迭代器\n",
    "\n",
    "    # Iter[np.array] => list(np.array)\n",
    "    dat = list(iter_arr) \n",
    "\n",
    "    # list(np.array) => matrix\n",
    "    if len(dat) < 1:  # Test zero iterator，万一给的迭代器为0\n",
    "        mat = np.array([])\n",
    "    else:\n",
    "        mat = np.vstack(dat) \n",
    "\n",
    "    # matrix => Iter[matrix]\n",
    "    yield mat # 返回迭代器用yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a55970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([4,5,6])\n",
    "v3 = np.array([7,8,9])\n",
    "v4 = np.array([10,11,12])\n",
    "np.vstack([v1,v2,v3,v4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286bcdc",
   "metadata": {},
   "source": [
    "变换后的结果依然是一个 RDD，但此时元素类型变成了矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4facbf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      "[]\n",
      "\n",
      "[array([], dtype=float64), array([[-1.085631,  0.997345,  0.282978, -1.506295, -0.5786  ],\n",
      "       [ 1.651437, -2.426679, -0.428913,  1.265936, -0.86674 ],\n",
      "       [-0.678886, -0.094709,  1.49139 , -0.638902, -0.443982],\n",
      "       [-0.434351,  2.20593 ,  2.186786,  1.004054,  0.386186],\n",
      "       [ 0.737369,  1.490732, -0.935834,  1.175829, -1.253881],\n",
      "       [-0.637752,  0.907105, -1.428681, -0.140069, -0.861755],\n",
      "       [-0.255619, -2.798589, -1.771533, -0.699877,  0.927462],\n",
      "       [-0.173636,  0.002846,  0.688223, -0.879536,  0.283627],\n",
      "       [-0.805367, -1.727669, -0.3909  ,  0.573806,  0.338589],\n",
      "       [-0.01183 ,  2.392365,  0.412912,  0.978736,  2.238143]]), array([[-1.294085, -1.038788,  1.743712, -0.798063,  0.029683],\n",
      "       [ 1.069316,  0.890706,  1.754886,  1.495644,  1.069393],\n",
      "       [-0.772709,  0.794863,  0.314272, -1.326265,  1.417299],\n",
      "       [ 0.807237,  0.04549 , -0.233092, -1.198301,  0.199524],\n",
      "       [ 0.468439, -0.831155,  1.162204, -1.097203, -2.1231  ],\n",
      "       [ 1.039727, -0.403366, -0.12603 , -0.837517, -1.605963],\n",
      "       [ 1.255237, -0.688869,  1.660952,  0.807308, -0.314758],\n",
      "       [-1.085902, -0.732462, -1.212523,  2.087113,  0.164441],\n",
      "       [ 1.150206, -1.267352,  0.181035,  1.177862, -0.335011],\n",
      "       [ 1.031114, -1.084568, -1.363472,  0.379401, -0.379176]])]\n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dat_p10 = file_p10.mapPartitions(part_to_mat) # 对每一个分区做一个映射的操作\n",
    "# 100个向量的RDD变成10个矩阵的RDD\n",
    "print(type(dat_p10))\n",
    "print()\n",
    "print(dat_p10.first()) # 边缘情况，第一个分区为0\n",
    "print()\n",
    "print(dat_p10.take(3))\n",
    "print()\n",
    "print(dat_p10.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587fdbb0",
   "metadata": {},
   "source": [
    "我们可以用 `filter()` 过滤掉空的分区："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecfacf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      "[[-1.085631  0.997345  0.282978 -1.506295 -0.5786  ]\n",
      " [ 1.651437 -2.426679 -0.428913  1.265936 -0.86674 ]\n",
      " [-0.678886 -0.094709  1.49139  -0.638902 -0.443982]\n",
      " [-0.434351  2.20593   2.186786  1.004054  0.386186]\n",
      " [ 0.737369  1.490732 -0.935834  1.175829 -1.253881]\n",
      " [-0.637752  0.907105 -1.428681 -0.140069 -0.861755]\n",
      " [-0.255619 -2.798589 -1.771533 -0.699877  0.927462]\n",
      " [-0.173636  0.002846  0.688223 -0.879536  0.283627]\n",
      " [-0.805367 -1.727669 -0.3909    0.573806  0.338589]\n",
      " [-0.01183   2.392365  0.412912  0.978736  2.238143]]\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dat_p10_nonempty = dat_p10.filter(lambda x: x.shape[0] > 0) # 过滤掉为0的RDD\n",
    "\n",
    "print(type(dat_p10_nonempty))\n",
    "print()\n",
    "print(dat_p10_nonempty.first())\n",
    "print()\n",
    "print(dat_p10_nonempty.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beec4f5",
   "metadata": {},
   "source": [
    "### 5. RDD 操作案例 - 求列和"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299cd239",
   "metadata": {},
   "source": [
    "np.array 版本的 RDD 求矩阵的列和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.246114436451575"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e9dc5",
   "metadata": {},
   "source": [
    "从输入矩阵文件开始，将操作串联："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f55faa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.map(str_to_vec).reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9226f55",
   "metadata": {},
   "source": [
    "使用分区版本的 RDD，先在每个分区上求列和："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f16bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.694266,  0.948677,  0.106428,  1.133682,  0.169049]),\n",
       " array([ 3.66858 , -4.315501,  3.881944,  0.689979, -1.877668]),\n",
       " array([-5.599247, -2.846053,  2.978673,  5.934997,  0.565914]),\n",
       " array([-5.60762 ,  0.977661, -5.157818, -2.868979, -0.570433]),\n",
       " array([-1.430094,  1.021662, -1.322512, -7.564743, -5.2081  ]),\n",
       " array([ 0.883702,  0.571757, -3.832934, -1.569966,  1.929   ]),\n",
       " array([ 0.070685,  4.474505, -0.598978,  2.525312,  0.210712])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_part = dat_p10_nonempty.map(lambda x: np.sum(x, axis=0))\n",
    "sum_part.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17068800",
   "metadata": {},
   "source": [
    "再将分区结果汇总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f2c006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.70826 ,  0.832708, -3.945197, -1.719718, -4.781526])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_part.reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a7d01",
   "metadata": {},
   "source": [
    "从输入矩阵文件开始，将操作串联："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.repartition(10).\\\n",
    "    mapPartitions(part_to_mat).\\\n",
    "    filter(lambda x: x.shape[0] > 0).\\\n",
    "    map(lambda x: np.sum(x, axis=0)).\\\n",
    "    reduce(lambda x1, x2: x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21b056",
   "metadata": {},
   "source": [
    "使用真实值检验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "231de070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.7082586 ,  0.83270703, -3.94519179, -1.71971787, -4.78152553])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caaf2e7",
   "metadata": {},
   "source": [
    "### 6. RDD 操作案例 - 矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684edfb9",
   "metadata": {},
   "source": [
    "模拟数据和真实值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6dcb844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.65326187,  0.43284335, -0.83326669,  1.65616556,  0.47393998, -1.20594195, -1.09926452,\n",
       "       -0.24483357, -0.58399139,  2.91984625, -1.22159268,  2.99167578,  0.04907967,  0.00526486,\n",
       "       -1.78033411, -1.03704672,  1.27253333,  0.0280204 ,  0.88785436,  0.03485989,  1.45756374,\n",
       "       -1.26733834,  0.89596346, -0.65027554,  1.24724097,  0.01338995, -0.45613812,  1.06057634,\n",
       "        0.33513133,  0.30420446, -1.8306843 ,  0.81135409,  0.8563569 , -0.59189289, -0.58993733,\n",
       "        0.85925493,  0.20665867, -2.07373852,  0.23232788, -2.69748055,  1.19285523, -0.22831252,\n",
       "       -0.75495708,  1.04599886, -0.59922216, -2.14049979, -0.68492854,  0.13322705,  0.11576237,\n",
       "       -1.07628496,  0.98308603,  2.28403745,  0.31327103,  0.97450293, -2.19087869, -1.38414598,\n",
       "       -2.06428815, -1.19693787, -2.20837322,  1.79393849,  0.37940968,  0.98364566,  2.12782768,\n",
       "        0.17228872, -1.42418937, -0.66160026,  0.20736396, -0.42352417, -1.83096405,  0.75557361,\n",
       "       -1.87660221, -1.93437067, -0.51802796,  0.70099077, -2.27776851, -0.17137795, -0.77013413,\n",
       "       -0.33715716, -0.46570004, -0.22885299,  0.07744646,  0.65965659,  1.30432415, -3.05410919,\n",
       "       -1.55812228, -0.35166363, -0.26695372,  1.71736731,  1.42907711,  0.74512303, -1.17590892,\n",
       "        1.28153134,  0.34006662,  1.1969479 ,  1.68259996, -2.70844742, -0.21291717,  2.74992919,\n",
       "       -2.1979523 ,  0.60576651])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "v = np.random.uniform(size=p)\n",
    "res = mat.dot(v)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2037ad4",
   "metadata": {},
   "source": [
    "np.array 版 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f47de328",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 27.0 failed 1 times, most recent failure: Lost task 3.0 in stage 27.0 (TID 88) (DESKTOP-189U52J executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 686, in main\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 678, in process\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13460\\2891024799.py\", line 1, in <lambda>\nAttributeError: 'float' object has no attribute 'dot'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 686, in main\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 678, in process\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13460\\2891024799.py\", line 1, in <lambda>\nAttributeError: 'float' object has no attribute 'dot'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res1 \u001b[38;5;241m=\u001b[39m \u001b[43mdat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m res1[:\u001b[38;5;241m10\u001b[39m]\n",
      "File \u001b[1;32mE:\\software\\spark\\python\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mE:\\software\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mE:\\software\\spark\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mE:\\software\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 27.0 failed 1 times, most recent failure: Lost task 3.0 in stage 27.0 (TID 88) (DESKTOP-189U52J executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 686, in main\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 678, in process\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13460\\2891024799.py\", line 1, in <lambda>\nAttributeError: 'float' object has no attribute 'dot'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 686, in main\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 678, in process\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"E:\\software\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13460\\2891024799.py\", line 1, in <lambda>\nAttributeError: 'float' object has no attribute 'dot'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "res1 = dat.map(lambda x: x.dot(v)).collect()\n",
    "res1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2acc05f",
   "metadata": {},
   "source": [
    "分区版 RDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733872af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1.65326236,  0.43284381, -0.83326654,  1.65616548,  0.47393997, -1.20594265, -1.09926439,\n",
       "        -0.24483374, -0.58399159,  2.91984624]),\n",
       " array([-1.22159275,  2.99167581,  0.04907979,  0.0052652 , -1.78033393, -1.03704719,  1.27253296,\n",
       "         0.02802034,  0.88785453,  0.03485997]),\n",
       " array([ 1.45756404, -1.26733862,  0.89596327, -0.65027561,  1.24724115,  0.01338989, -0.45613776,\n",
       "         1.06057673,  0.33513193,  0.30420455,  2.28403732,  0.31327091,  0.97450361, -2.19087935,\n",
       "        -1.38414658, -2.06428804, -1.19693768, -2.20837397,  1.79393855,  0.37941031]),\n",
       " array([-1.8306849 ,  0.81135346,  0.85635656, -0.59189308, -0.58993783,  0.8592545 ,  0.20665878,\n",
       "        -2.07373867,  0.23232755, -2.69748044,  0.9836457 ,  2.12782845,  0.17228866, -1.42418964,\n",
       "        -0.66160031,  0.20736295, -0.4235236 , -1.83096434,  0.75557361, -1.87660252]),\n",
       " array([ 1.19285543, -0.22831212, -0.75495698,  1.04599886, -0.59922233, -2.14049959, -0.68492885,\n",
       "         0.13322687,  0.11576229, -1.07628444, -1.93437101, -0.51802806,  0.70099126, -2.27776847,\n",
       "        -0.17137845, -0.77013423, -0.33715737, -0.46569988, -0.22885317,  0.07744686]),\n",
       " array([ 0.98308645,  0.65965705,  1.30432399, -3.05410973, -1.55812232, -0.35166336, -0.26695394,\n",
       "         1.7173679 ,  1.42907711,  0.74512261, -1.17590882]),\n",
       " array([ 1.28153159,  0.34006666,  1.19694819,  1.68260023, -2.70844726, -0.21291761,  2.74992875,\n",
       "        -2.1979517 ,  0.60576649])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_part = dat_p10_nonempty.map(lambda x: x.dot(v)).collect()\n",
    "res_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd0404",
   "metadata": {},
   "source": [
    "拼接分区结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "335b21e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.65326236,  0.43284381, -0.83326654,  1.65616548,  0.47393997, -1.20594265, -1.09926439,\n",
       "       -0.24483374, -0.58399159,  2.91984624, -1.22159275,  2.99167581,  0.04907979,  0.0052652 ,\n",
       "       -1.78033393, -1.03704719,  1.27253296,  0.02802034,  0.88785453,  0.03485997,  1.45756404,\n",
       "       -1.26733862,  0.89596327, -0.65027561,  1.24724115,  0.01338989, -0.45613776,  1.06057673,\n",
       "        0.33513193,  0.30420455,  2.28403732,  0.31327091,  0.97450361, -2.19087935, -1.38414658,\n",
       "       -2.06428804, -1.19693768, -2.20837397,  1.79393855,  0.37941031, -1.8306849 ,  0.81135346,\n",
       "        0.85635656, -0.59189308, -0.58993783,  0.8592545 ,  0.20665878, -2.07373867,  0.23232755,\n",
       "       -2.69748044,  0.9836457 ,  2.12782845,  0.17228866, -1.42418964, -0.66160031,  0.20736295,\n",
       "       -0.4235236 , -1.83096434,  0.75557361, -1.87660252,  1.19285543, -0.22831212, -0.75495698,\n",
       "        1.04599886, -0.59922233, -2.14049959, -0.68492885,  0.13322687,  0.11576229, -1.07628444,\n",
       "       -1.93437101, -0.51802806,  0.70099126, -2.27776847, -0.17137845, -0.77013423, -0.33715737,\n",
       "       -0.46569988, -0.22885317,  0.07744686,  0.98308645,  0.65965705,  1.30432399, -3.05410973,\n",
       "       -1.55812232, -0.35166336, -0.26695394,  1.7173679 ,  1.42907711,  0.74512261, -1.17590882,\n",
       "        1.28153159,  0.34006666,  1.19694819,  1.68260023, -2.70844726, -0.21291761,  2.74992875,\n",
       "       -2.1979517 ,  0.60576649])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(res_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea100bd",
   "metadata": {},
   "source": [
    "关闭 Spark 连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6e9f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
